{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79f03f87",
   "metadata": {},
   "source": [
    "DAY 1 – Load Text Dataset & Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e11a0c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 11314\n",
      "Target classes: ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware']\n",
      "\n",
      "Sample text:\n",
      " From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a m\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "data = fetch_20newsgroups(subset='train')\n",
    "\n",
    "print(\"Number of documents:\", len(data.data))\n",
    "print(\"Target classes:\", data.target_names[:5])\n",
    "print(\"\\nSample text:\\n\", data.data[0][:500])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea847b9",
   "metadata": {},
   "source": [
    "DAY 2 – Text Cleaning & Preprocessing (Basic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "486dc8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\W+', ' ', text)\n",
    "    return text\n",
    "\n",
    "cleaned_texts = [clean_text(text) for text in data.data]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a76842",
   "metadata": {},
   "source": [
    "DAY 3 – Bag of Words (CountVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cb4e4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (11314, 5000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english', max_features=5000)\n",
    "X = vectorizer.fit_transform(cleaned_texts)\n",
    "\n",
    "y = data.target\n",
    "\n",
    "print(\"Feature matrix shape:\", X.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c407380",
   "metadata": {},
   "source": [
    "DAY 4 – Train/Test Split + Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "942170d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8214759169244366\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "pred = nb.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f3f885",
   "metadata": {},
   "source": [
    "DAY 5 – Model Evaluation (Classification Report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb117218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.93      0.90        97\n",
      "           1       0.58      0.87      0.69       104\n",
      "           2       1.00      0.03      0.07       115\n",
      "           3       0.54      0.74      0.62       123\n",
      "           4       0.71      0.87      0.78       126\n",
      "           5       0.81      0.83      0.82       106\n",
      "           6       0.69      0.85      0.76       109\n",
      "           7       0.86      0.89      0.88       139\n",
      "           8       0.85      0.91      0.88       122\n",
      "           9       0.88      0.96      0.92       102\n",
      "          10       1.00      0.93      0.96       108\n",
      "          11       1.00      0.89      0.94       125\n",
      "          12       0.81      0.76      0.79       114\n",
      "          13       0.96      0.91      0.93       119\n",
      "          14       0.93      0.89      0.91       127\n",
      "          15       0.92      0.88      0.90       122\n",
      "          16       0.90      0.91      0.91       121\n",
      "          17       0.98      0.86      0.92       102\n",
      "          18       0.81      0.82      0.82       107\n",
      "          19       0.80      0.65      0.72        75\n",
      "\n",
      "    accuracy                           0.82      2263\n",
      "   macro avg       0.85      0.82      0.81      2263\n",
      "weighted avg       0.85      0.82      0.81      2263\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76af4ea2",
   "metadata": {},
   "source": [
    "DAY 6 – TF-IDF Vectorization (Improvement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ac58350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Accuracy: 0.8590366769774636\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "X_tfidf = tfidf.fit_transform(cleaned_texts)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_tfidf, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "nb_tfidf = MultinomialNB()\n",
    "nb_tfidf.fit(X_train, y_train)\n",
    "\n",
    "pred_tfidf = nb_tfidf.predict(X_test)\n",
    "print(\"TF-IDF Accuracy:\", accuracy_score(y_test, pred_tfidf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb02b660",
   "metadata": {},
   "source": [
    "DAY 7 – Predict on New Custom Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51455ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Category: sci.space\n"
     ]
    }
   ],
   "source": [
    "sample_text = [\"NASA launches new satellite into space\"]\n",
    "\n",
    "sample_clean = [clean_text(sample_text[0])]\n",
    "sample_vec = tfidf.transform(sample_clean)\n",
    "\n",
    "prediction = nb_tfidf.predict(sample_vec)\n",
    "print(\"Predicted Category:\", data.target_names[prediction[0]])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
